avg_size,edge_collisions,entropy_loss,episodes,policy_entropy,policy_loss,reward_rate,self_collisions,steps,t,value_loss
3.103515625,0.08203125,-1.3842475414276123,4299,1.3843533992767334,0.2023990899324417,-5.891659736633301,0.001953125,51200,2.9803311824798584,0.3402957320213318
